---
title: "Predicting Barbell Lifts Performance from Accelerometers Data"
author: "Ph A"
date: "19 septembre 2016"
output: 
  html_document: 
    keep_md: yes
    toc: yes
    number_sections: yes
    
references:
- id: walker01
  title: Random Forest Algorithms.
  author:
  - family: Walker
    given: Michael
  container-title: Data Science Central
  URL: '[[LINK]](http://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm)'
  issued:
    year: 2013
    month: 9
    day: 24
- id: cutler01
  title: Trees and Random Forests.
  author:
  - family: Cutler
    given: Adele
  container-title: Utah State University
  URL: '[[LINK]](http://www.math.usu.edu/adele/RandomForests/UofU2013.pdf)'
  page: 73
  issued:
    year: 2013
    month: 10
    day: 3
- id: strobl01
  title: Conditional variable importance for random forests.
  author:
  - family: Strobl
    given: Carolin
  - family: Boulesteix
    given: Anne-Laure
  - family: Kneib
    given: Thomas
  - family: Augustin
    given: Thomas
  - family: Zeileis
    given: Achim
  container-title: BMC Bioinformatics
  URL: '[[LINK]](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307)'
  page: 73
  issued:
    year: 2008
    month: 8
- id: greski01
  title: Improving the Performance of Random Forest Models with Parallel Processing.
  author:
  - family: Greski
    given: Leonard
  container-title: Practical Machine Learning Coursera Forum
  URL: '[[LINK]](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md)'
  issued:
    year: 2016
    month: 5
- id: velloso01
  title: Qualitative Activity Recognition of Weight Lifting Exercises
  author:
  - family: Velloso
    given: E.
  - family: Bulling
    given: A.
  - family: Gellersen
    given: H.
  - family: Ugulino
    given: W.
  - family: Fuks
    given: H.
  container-title: Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) - Stuttgart, Germany - ACM SIGCHI, 2013.
  URL: '[[LINK]](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)'
  type: article-journal
  issued:
    year: 2013
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path='FIGURES/', echo=TRUE)
```

**WARNING** : we submitted here (following up on the mentor post) the gh-page (https://palcouffe.github.io/practicalmachinelearning/) but the full repository (with .Rmd, .md and .html) can be found at https://github.com/palcouffe/practicalmachinelearning.

# Synopsis

The goal of this project is to use data collected from accelerometers placed on the belt, forearm, arm and dumbell of 6 participants performing barbell lifts to predict how well they did the exercices. The participants were asked to perform the lifts in a correct way and in 4 incorrect ways.
In a first step, we loaded, explored and preprocessed the data. We then used a Random Forest Classification Algorithm (and justified our choice) to build and train a model. Using cross-validation, we computed the accuracy of that model and the expected out of sample error. Finally, our prediction model was applied to 20 different test cases.

# Loading and Preprocessing Data
## Software environment used

```{r libraries_used, message=FALSE}
library(caret)                          # obviously ...
library(caretEnsemble)                  # to easily extract metrics like accuracy from any caret model
library(parallel)                       # for parallel processing
library(doParallel)                     # for parallel processing
library(formattable)                    # for the percent formatting function
```

## Downloading and Preliminary Exploration

### Downloading and Reading the Data

```{r download_read, cache=TRUE}
if (!file.exists("DATA")) {dir.create("DATA")}                                             # create dir for data
fileUrlTraining <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"  # training data url
download.file(fileUrlTraining,destfile="./DATA/pml-training.csv", method="curl")           # download the file

fileUrlTesting <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"    # testing data url
download.file(fileUrlTesting,destfile="./DATA/pml-testing.csv", method="curl")             # download the file   

rawTrainingData <- read.csv("./DATA/pml-training.csv")                                     # read training data
rawTestingData <- read.csv("./DATA/pml-testing.csv")                                       # read testing data 
```

### Preliminary Exploration of the Raw Data

```{r explore_data_a}
dim(rawTrainingData)                                  # a very preliminary look up
```

We have `r ncol(rawTrainingData)` variables and `r nrow(rawTrainingData)` observations. Let's explore the first 15 variables to see if we can refine our reading of the data.

```{r explore_data_b}
str(rawTrainingData[,c(1:15)], vec.len=2)            # look up 15 first variables
```

It appears clearly that :

*   **the first 7 rows** do not correspond to measurements and data captured but qualify the participants (e.g. `user_name`) or are descriptives about when the measurements was done (e.g. `raw_timestamp_part_1` or `num_window`). We will therefore not keep those variables as predictors
*   **"" and "#DIV/0!"** should be imported as NAs. We will therefore reimport our data specifying these to be translated into NAs.

## Preprocessing and Predictors picking

### Preliminary cleaning
Following up on our preliminary exploration, we can now reload the data excluding the first 7 variables and importing  "" and "#DIV/0!" as NAs.

```{r read_data2}
trainingData <- read.csv("./DATA/pml-training.csv", 
                         na.strings=c("NA","","#DIV/0!"))[,-c(1:7)]      # read training data
testingData <- read.csv("./DATA/pml-testing.csv",
                        na.strings=c("NA","","#DIV/0!"))[,-c(1:7)]       # read testing data 
```

We now should no longer have any character variables and only one factor variable.

```{r explore_data_c}
str(Filter(is.factor,trainingData), vec.len=2)                           # filtering to get the factors left
typeVar <- unique(sapply(trainingData,typeof))                           # listing type of variables in data
typeVar
```

We indeed have now only variables of type `r typeVar` and the factor variable `classe` with 5 levels. This is indeed the variable we will need to predict and it takes 5 values (cf documentation @velloso01 [[LINK]](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)) each of which indicates how well the lift was performed :

*    A : exactly according to the specification
*    B : throwing the elbows to the front
*    C : lifting the dumbbell only halfway
*    D : lowering the dumbbell only halfway
*    E : throwing the hips to the front

### Dealing with NA's

We will now explore if we should impute values for those NAs or just not pick up the variables with a too important percentage of NAs. To take our decision, we computed a table showing the number of variables vs. percentage of NAs they include.

```{r checking_NAs}
pctgNAtraining <- colSums(is.na(trainingData)) / nrow(trainingData)     # computing pctg NAs for training data
tbl <- table(percent(pctgNAtraining,0))                                 # display number of var vs pctg NAs
knitr::kable(data.frame(tbl), 
             col.names=c("% NAs in Variable", "Number of Variables"))   # pretty print
```

It appears that the minumum percentage of NAs is `r percent(min(pctgNAtraining[pctgNAtraining!=0]))`. We can therefore safely exclude from our predictors any variable that has NAs. We will eliminate those variables from the training data set and eliminate the same variables from the test set.

```{r eliminating_NAs}
trainingData <- trainingData[,pctgNAtraining ==0]                       # keeping only variables with no NAs
testingData <- testingData[,pctgNAtraining ==0]                         # keeping same variables in testing
```

###Identifying Zero- and Near Zero-Variance Predictors

Let's examine if we have any variables displaying near zero-variance that would need not to be picked up.

```{r zerovariance}
nzvTraining <- nearZeroVar(trainingData, saveMetrics=TRUE)               # computing nzv for variables
nrow(nzvTraining[nzvTraining$nzv==TRUE,])                                # displaying # variables with nzv
```

It appears that we have `r nrow(nzvTraining[nzvTraining$nzv==TRUE,])` variable with zero or near-zero variance in the data set once variables with NAs have been eliminated.

###Identifying Highly Correlated Predictors
To identify Predictors that are highly correlated, let's build a correlation matrix and find the variables with a correlation above `0.95`. We will operate on the training data without the `classe` variable (the last variable).

```{r correlated_predictors}
corTraining <- cor(trainingData[,-ncol(trainingData)])
highlyCorrelatedTraining <- findCorrelation(corTraining, cutoff = 0.95)
names(trainingData)[highlyCorrelatedTraining]
```
We found `r length(highlyCorrelatedTraining)` variables (`r names(trainingData)[highlyCorrelatedTraining]`) that are highly correlated. However, we decided at this stage not to remove those from our predictors. The reason is that we decided to choose a Random Forest Algorithm and this algorithm is reputed to nicely handle highly correlated predictors (cf Background section in  @strobl01 [[LINK]](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307)))

> "Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables."

## Final Set of Predictors selected
```{r predictors_selected}
dim(trainingData[,-ncol(trainingData)])
```

To summarize, our final set of predictors will include `r ncol(trainingData[,-ncol(trainingData)])` variables from the original data set and `r nrow(trainingData[,-ncol(trainingData)])` observations.

# Partitionning into Training, Validation and Testing Sets
We already have a testing set but this does not include the actual `classe` value so it can't be used to compute an out of sample error.
We will therefore split our training set into 2 sets, a **set to train our model** and a **validation set** that will be used to compute Predicted Accuracy and Out of Sample Error (using cross validation on the validation set).

```{r partition_trainingdata}
set.seed(95104)
inTrain=createDataPartition(trainingData$classe,p=3/4)[[1]]
myTrainingData = trainingData[inTrain,]
myValidationData = trainingData[-inTrain,]
```

# Training a Random Forest Model with Parallel Processing

The variable to predict is `classe`, a factor of 5 values. We are in a Classification problem for which a Random Forest Algorithm is particularly well adapted. There are many advantages to this algorithm including the ability to deal with hightly correlated predictors that make us think it was well suited for our study case. We would like to quote here from :

*   @walker01 [[LINK]](http://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm) : 

        *    Accuracy
        *    Runs efficiently on large data bases
        *    Handles thousands of input variables without variable deletion
        *    Provides effective methods for estimating missing data
        *    Maintains accuracy when a large proportion of the data are missing
*   @cutler01 [[LINK]](http://www.math.usu.edu/adele/RandomForests/UofU2013.pdf) : 

        *    Quick to fit, even for large problems
        *    No formal distributional assumptions
        *    Automatically fits hightly non-linear interactions
        *    Automatic variable selection

We will train our model using a K-fold (with 10 folds) cross validation as enabled by the caret package and for performance speed up will use a parallel processing, following the steps kindly described by our mentor Leonard Greski in @greski01 [[LINK]](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md).

```{r set_parallel_processing}
cluster <- makeCluster(detectCores() - 1)               # convention to leave 1 core for OS
registerDoParallel(cluster)
```

```{r train_model, cache=TRUE, message=FALSE}
set.seed(95104)

x <- myTrainingData[,-53]                               # set the predictors
y <- myTrainingData[,53]                                # set the variable to predict

fitControl <- trainControl(method = "cv",               # set cross validation
                           number = 10,                 # to be a 10 folds
                           allowParallel = TRUE)        # using parallelization

modFit <- train(x,y,method="rf",                        # fit model on training data
                 data=myTrainingData,
                 trControl= fitControl)

stopCluster(cluster)
```

Let's display the characteristics of the trained model (the best random forest will be picked up as the final model) :
```{r show_accuracy}
modFit
```

We observe a model accuracy of **`r percent(getMetric(modFit))`** which is very good. 

# Cross Validation
We can now use the validation set we put aside to predict on and estimate the Expected Predictive Accuracy and Out of Sample Error.

## Predicting on Validation Data
```{r cross_validation, message=FALSE}
predictMod <- predict(modFit, newdata=myValidationData)         # apply model on validation data
```

## Accuracy of the Model and Expected Out of Sample error
Let's build our confusion matrix to compute the Expected Predictive Accuracy and deduce the Out of Sample Error (1 - accuracy).
```{r accuracy}
confusionMatrix(predictMod,myValidationData$classe)             # build confusion matrix
rfAccuracy <- confusionMatrix(predictMod,myValidationData$classe)$overall['Accuracy']
rfAccuracy
```

Accuracy is **`r percent(rfAccuracy)`** and Expected Out of Sample error **`r percent(1 - rfAccuracy)`**.
 
## Variable Importance
As a complement, we display here the top 10 variables by importance in the Random Forest Algorithm showing the contribution of these variables to the model.
```{r display_varimp}
varImpPlot(modFit$finalModel, n.var=10, main="Top 10 Variable Importance")      # plot variable importance
```

# Predicting on Testing Set
Finally, we can now use our testing set to predict on (and feed in the quizz).
```{r predict_testing}
finalpredict <- predict(modFit, newdata=testingData)                            # apply model on test data
tbl <- data.frame(matrix(finalpredict,ncol=20,byrow=TRUE))
knitr::kable(data.frame(tbl),                                                   # pretty print predictions
             col.names=c(1:20)) 
```

The quizz resulted in a 20/20 ! Yeah !!! 

# References

<style type="text/css">

h1 { /* Header 1 */
 font-size: 28px;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}

table {
   padding: 0;border-collapse: collapse; }
table tr {
   border-top: 1px solid #cccccc;
   background-color: white;
   margin: 0;
   padding: 0; }
table tr:nth-child(2n) {
   background-color: #dfdfdf; }
table tr th {
   background-color: #c6c6c6;
   font-weight: bold;
   text-align:center;
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr td {
   border: 1px solid #cccccc;
   margin: 0;
   text-align:center;
   padding: 6px 13px;}
   
table tr th :first-child, table tr td :first-child {
   margin-top: 0; }
table tr th :last-child, table tr td :last-child {
   margin-bottom: 0; }

blockquote p {
  font-size: 14px;
  font-style: italic;
}

</style>


